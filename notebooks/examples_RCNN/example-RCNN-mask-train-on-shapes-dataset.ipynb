{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is based off of [matterport's train_shapes.ipynb example](https://github.com/matterport/Mask_RCNN/blob/master/samples/shapes/train_shapes.ipynb)\n",
    "\n",
    "The notebook shows how to train Mask R-CNN on your own dataset. To keep things simple we use a synthetic dataset of shapes (squares, triangles, and circles) which enables fast training. You'd still need a GPU, though, because the network backbone is a Resnet101, which would be too slow to train on a CPU. On a GPU, you can start to get okay-ish results in a few minutes, and good results in less than an hour.\n",
    "\n",
    "The code of the *Shapes* dataset is included below. It generates images on the fly, so it doesn't require downloading any data. And it can generate images of any size, so we pick a small image size to train faster. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install Libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade JLpyUtils\n",
    "import IPython.display\n",
    "IPython.display.clear_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "import skimage.io\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import importlib\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.insert(0, '/mr3_boltprod_john_t_leonard/Data_Science_Projects./ObjDetection/pyDSlib/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyDSlib as DS\n",
    "import pyDSlib.ML.NeuralNet as NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(NN.RCNN.mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Configurations:\n",
      "BACKBONE                       resnet101\n",
      "BACKBONE_STRIDES               [4, 8, 16, 32, 64]\n",
      "BATCH_SIZE                     8\n",
      "BBOX_STD_DEV                   [0.1 0.1 0.2 0.2]\n",
      "COMPUTE_BACKBONE_SHAPE         None\n",
      "DETECTION_MAX_INSTANCES        100\n",
      "DETECTION_MIN_CONFIDENCE       0.7\n",
      "DETECTION_NMS_THRESHOLD        0.3\n",
      "FPN_CLASSIF_FC_LAYERS_SIZE     1024\n",
      "GPU_COUNT                      1\n",
      "GRADIENT_CLIP_NORM             5.0\n",
      "IMAGES_PER_GPU                 8\n",
      "IMAGE_CHANNEL_COUNT            3\n",
      "IMAGE_MAX_DIM                  128\n",
      "IMAGE_META_SIZE                16\n",
      "IMAGE_MIN_DIM                  128\n",
      "IMAGE_MIN_SCALE                0\n",
      "IMAGE_RESIZE_MODE              square\n",
      "IMAGE_SHAPE                    [128 128   3]\n",
      "LEARNING_MOMENTUM              0.9\n",
      "LEARNING_RATE                  0.001\n",
      "LOSS_WEIGHTS                   {'rpn_class_loss': 1.0, 'rpn_bbox_loss': 1.0, 'mrcnn_class_loss': 1.0, 'mrcnn_bbox_loss': 1.0, 'mrcnn_mask_loss': 1.0}\n",
      "MASK_POOL_SIZE                 14\n",
      "MASK_SHAPE                     [28, 28]\n",
      "MAX_GT_INSTANCES               100\n",
      "MEAN_PIXEL                     [123.7 116.8 103.9]\n",
      "MINI_MASK_SHAPE                (56, 56)\n",
      "NAME                           shapes\n",
      "NUM_CLASSES                    4\n",
      "POOL_SIZE                      7\n",
      "POST_NMS_ROIS_INFERENCE        1000\n",
      "POST_NMS_ROIS_TRAINING         2000\n",
      "PRE_NMS_LIMIT                  6000\n",
      "ROI_POSITIVE_RATIO             0.33\n",
      "RPN_ANCHOR_RATIOS              [0.5, 1, 2]\n",
      "RPN_ANCHOR_SCALES              (8, 16, 32, 64, 128)\n",
      "RPN_ANCHOR_STRIDE              1\n",
      "RPN_BBOX_STD_DEV               [0.1 0.1 0.2 0.2]\n",
      "RPN_NMS_THRESHOLD              0.7\n",
      "RPN_TRAIN_ANCHORS_PER_IMAGE    256\n",
      "STEPS_PER_EPOCH                100\n",
      "TOP_DOWN_PYRAMID_SIZE          256\n",
      "TRAIN_BN                       False\n",
      "TRAIN_ROIS_PER_IMAGE           32\n",
      "USE_MINI_MASK                  True\n",
      "USE_RPN_ROIS                   True\n",
      "VALIDATION_STEPS               5\n",
      "WEIGHT_DECAY                   0.0001\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class ShapesConfig(NN.RCNN.mask.Config):\n",
    "    \"\"\"\n",
    "    Configuration for training on the toy shapes dataset.\n",
    "    Derives from the base Config class and overrides values specific\n",
    "    to the toy shapes dataset.\n",
    "    \"\"\"\n",
    "    # Give the configuration a recognizable name\n",
    "    NAME = \"shapes\"\n",
    "\n",
    "    # Train on 1 GPU and 8 images per GPU. We can put multiple images on each\n",
    "    # GPU because the images are small. Batch size is 8 (GPUs * images/GPU).\n",
    "    GPU_COUNT = 1\n",
    "    IMAGES_PER_GPU = 8\n",
    "\n",
    "    # Number of classes (including background)\n",
    "    NUM_CLASSES = 1 + 3  # background + 3 shapes\n",
    "\n",
    "    # Use small images for faster training. Set the limits of the small side\n",
    "    # the large side, and that determines the image shape.\n",
    "    IMAGE_MIN_DIM = 128\n",
    "    IMAGE_MAX_DIM = 128\n",
    "\n",
    "    # Use smaller anchors because our image and objects are small\n",
    "    RPN_ANCHOR_SCALES = (8, 16, 32, 64, 128)  # anchor side in pixels\n",
    "\n",
    "    # Reduce training ROIs per image because the images are small and have\n",
    "    # few objects. Aim to allow ROI sampling to pick 33% positive ROIs.\n",
    "    TRAIN_ROIS_PER_IMAGE = 32\n",
    "\n",
    "    # Use a small epoch since the data is simple\n",
    "    STEPS_PER_EPOCH = 100\n",
    "\n",
    "    # use small validation steps since the epoch is small\n",
    "    VALIDATION_STEPS = 5\n",
    "    \n",
    "config = ShapesConfig()\n",
    "config.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook Preferences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ax(rows=1, cols=1, size=8):\n",
    "    \"\"\"Return a Matplotlib Axes array to be used in\n",
    "    all visualizations in the notebook. Provide a\n",
    "    central point to control graph sizes.\n",
    "    \n",
    "    Change the default size attribute to control the size\n",
    "    of rendered images\n",
    "    \"\"\"\n",
    "    _, ax = plt.subplots(rows, cols, figsize=(size*cols, size*rows))\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "Create a synthetic dataset\n",
    "\n",
    "Extend the Dataset class and add a method to load the shapes dataset, `load_shapes()`, and override the following methods:\n",
    "\n",
    "* load_image()\n",
    "* load_mask()\n",
    "* image_reference()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShapesDataset(NN.RCNN.mask.utils.Dataset):\n",
    "    \"\"\"Generates the shapes synthetic dataset. The dataset consists of simple\n",
    "    shapes (triangles, squares, circles) placed randomly on a blank surface.\n",
    "    The images are generated on the fly. No file access required.\n",
    "    \"\"\"\n",
    "\n",
    "    def load_shapes(self, count, height, width):\n",
    "        \"\"\"Generate the requested number of synthetic images.\n",
    "        count: number of images to generate.\n",
    "        height, width: the size of the generated images.\n",
    "        \"\"\"\n",
    "        # Add classes\n",
    "        self.add_class(\"shapes\", 1, \"square\")\n",
    "        self.add_class(\"shapes\", 2, \"circle\")\n",
    "        self.add_class(\"shapes\", 3, \"triangle\")\n",
    "\n",
    "        # Add images\n",
    "        # Generate random specifications of images (i.e. color and\n",
    "        # list of shapes sizes and locations). This is more compact than\n",
    "        # actual images. Images are generated on the fly in load_image().\n",
    "        for i in range(count):\n",
    "            bg_color, shapes = self.random_image(height, width)\n",
    "            self.add_image(\"shapes\", image_id=i, path=None,\n",
    "                           width=width, height=height,\n",
    "                           bg_color=bg_color, shapes=shapes)\n",
    "\n",
    "    def load_image(self, image_id):\n",
    "        \"\"\"Generate an image from the specs of the given image ID.\n",
    "        Typically this function loads the image from a file, but\n",
    "        in this case it generates the image on the fly from the\n",
    "        specs in image_info.\n",
    "        \"\"\"\n",
    "        info = self.image_info[image_id]\n",
    "        bg_color = np.array(info['bg_color']).reshape([1, 1, 3])\n",
    "        image = np.ones([info['height'], info['width'], 3], dtype=np.uint8)\n",
    "        image = image * bg_color.astype(np.uint8)\n",
    "        for shape, color, dims in info['shapes']:\n",
    "            image = self.draw_shape(image, shape, dims, color)\n",
    "        return image\n",
    "\n",
    "    def image_reference(self, image_id):\n",
    "        \"\"\"Return the shapes data of the image.\"\"\"\n",
    "        info = self.image_info[image_id]\n",
    "        if info[\"source\"] == \"shapes\":\n",
    "            return info[\"shapes\"]\n",
    "        else:\n",
    "            super(self.__class__).image_reference(self, image_id)\n",
    "\n",
    "    def load_mask(self, image_id):\n",
    "        \"\"\"Generate instance masks for shapes of the given image ID.\n",
    "        \"\"\"\n",
    "        info = self.image_info[image_id]\n",
    "        shapes = info['shapes']\n",
    "        count = len(shapes)\n",
    "        mask = np.zeros([info['height'], info['width'], count], dtype=np.uint8)\n",
    "        for i, (shape, _, dims) in enumerate(info['shapes']):\n",
    "            mask[:, :, i:i+1] = self.draw_shape(mask[:, :, i:i+1].copy(),\n",
    "                                                shape, dims, 1)\n",
    "        # Handle occlusions\n",
    "        occlusion = np.logical_not(mask[:, :, -1]).astype(np.uint8)\n",
    "        for i in range(count-2, -1, -1):\n",
    "            mask[:, :, i] = mask[:, :, i] * occlusion\n",
    "            occlusion = np.logical_and(occlusion, np.logical_not(mask[:, :, i]))\n",
    "        # Map class names to class IDs.\n",
    "        class_ids = np.array([self.class_names.index(s[0]) for s in shapes])\n",
    "        return mask.astype(np.bool), class_ids.astype(np.int32)\n",
    "\n",
    "    def draw_shape(self, image, shape, dims, color):\n",
    "        \"\"\"Draws a shape from the given specs.\"\"\"\n",
    "        # Get the center x, y and the size s\n",
    "        x, y, s = dims\n",
    "        if shape == 'square':\n",
    "            cv2.rectangle(image, (x-s, y-s), (x+s, y+s), color, -1)\n",
    "        elif shape == \"circle\":\n",
    "            cv2.circle(image, (x, y), s, color, -1)\n",
    "        elif shape == \"triangle\":\n",
    "            points = np.array([[(x, y-s),\n",
    "                                (x-s/math.sin(math.radians(60)), y+s),\n",
    "                                (x+s/math.sin(math.radians(60)), y+s),\n",
    "                                ]], dtype=np.int32)\n",
    "            cv2.fillPoly(image, points, color)\n",
    "        return image\n",
    "\n",
    "    def random_shape(self, height, width):\n",
    "        \"\"\"Generates specifications of a random shape that lies within\n",
    "        the given height and width boundaries.\n",
    "        Returns a tuple of three valus:\n",
    "        * The shape name (square, circle, ...)\n",
    "        * Shape color: a tuple of 3 values, RGB.\n",
    "        * Shape dimensions: A tuple of values that define the shape size\n",
    "                            and location. Differs per shape type.\n",
    "        \"\"\"\n",
    "        # Shape\n",
    "        shape = random.choice([\"square\", \"circle\", \"triangle\"])\n",
    "        # Color\n",
    "        color = tuple([random.randint(0, 255) for _ in range(3)])\n",
    "        # Center x, y\n",
    "        buffer = 20\n",
    "        y = random.randint(buffer, height - buffer - 1)\n",
    "        x = random.randint(buffer, width - buffer - 1)\n",
    "        # Size\n",
    "        s = random.randint(buffer, height//4)\n",
    "        return shape, color, (x, y, s)\n",
    "\n",
    "    def random_image(self, height, width):\n",
    "        \"\"\"Creates random specifications of an image with multiple shapes.\n",
    "        Returns the background color of the image and a list of shape\n",
    "        specifications that can be used to draw the image.\n",
    "        \"\"\"\n",
    "        # Pick random background color\n",
    "        bg_color = np.array([random.randint(0, 255) for _ in range(3)])\n",
    "        # Generate a few random shapes and record their\n",
    "        # bounding boxes\n",
    "        shapes = []\n",
    "        boxes = []\n",
    "        N = random.randint(1, 4)\n",
    "        for _ in range(N):\n",
    "            shape, color, dims = self.random_shape(height, width)\n",
    "            shapes.append((shape, color, dims))\n",
    "            x, y, s = dims\n",
    "            boxes.append([y-s, x-s, y+s, x+s])\n",
    "        # Apply non-max suppression wit 0.3 threshold to avoid\n",
    "        # shapes covering each other\n",
    "        keep_ixs = NN.RCNN.mask.utils.non_max_suppression(np.array(boxes), np.arange(N), 0.3)\n",
    "        shapes = [s for i, s in enumerate(shapes) if i in keep_ixs]\n",
    "        return bg_color, shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training dataset\n",
    "dataset_train = ShapesDataset()\n",
    "dataset_train.load_shapes(500, config.IMAGE_SHAPE[0], config.IMAGE_SHAPE[1])\n",
    "dataset_train.prepare()\n",
    "\n",
    "# Validation dataset\n",
    "dataset_val = ShapesDataset()\n",
    "dataset_val.load_shapes(50, config.IMAGE_SHAPE[0], config.IMAGE_SHAPE[1])\n",
    "dataset_val.prepare()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1008x360 with 5 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAzAAAACqCAYAAAByHaqjAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAACztJREFUeJzt3VuIV1XDx/GfjmmWZo/ZpHbRyRLShJTGMUcdbUjUMJOgoJOmHUyhojA8lUFlb5QaWiSkHRSMDlKWpShlpZkdsC7UTooXhRrhI6SISDPvRTS8kmW+PD7jYj6fu9n//6z/2nvWzfe/2HtaNDQ0NAQAAKAALZt6AgAAAP+UgAEAAIohYAAAgGIIGAAAoBgCBgAAKIaAAQAAitHsA2bHjh2pq6s77Fi3bt2OeZxhw4Zl06ZNSZJ33303HTt2zB9PqJ48eXIWL1581DFmzJiRc84557D5bNq0Kf3798/AgQMzZMiQbN++PUmyffv2DBw4MLW1tRk8eHB+/PHHvxx327Zt6dOnT9q1a5d169Y1Hr/nnntSXV2d6urqPP74443HZ82alcsuuyxVVVWZPXv2sV0IAAA4jpp9wPyn1NTUZP369UmS9evXp3fv3tm8eXPjzwMGDDjqGHfddVc++OCDw4516dIlK1euzEcffZT7778/Dz30UJLk2Wefzbhx47J27drccsstmTdv3l+O26VLl6xevTrXXnvtYccnTpyYTz/9NJ988kneeuutbNu2Lb/++msWLVrUePy5557L/v37j+la0Dz89ttvTT0FAKAZEjD/0IQJE/Lyyy+nvr4+Q4cOzcaNGw97vaampnF34+uvv86ECROybt26HDx4MLt27cq555571M/o0qVLWrY8/E/SuXPntG/fPknSunXrtGrVKknSo0eP7N27N0myZ8+eVFZW5uDBg6mpqck333yT3bt3p6qqKnv37s0pp5ySjh07/unzLrzwwiRJy5YtU1FRkYqKirRt2zZdu3bNgQMHcuDAgbRt2zYnnXTSsV0sTgibN29Ov379Mnjw4AwbNixbtmxJVVVVRowYkZtvvjkzZ85McviO4/jx47N27dokydChQ1NbW5uqqqps2LAhSTJz5syMGTMmI0eOzKuvvpoPP/wwgwYNSm1tbe688874v7gAwPHWqqkncCL48ssvU1tb+7fvmTNnToYMGZL169fniiuuSN++fQ97vW/fvrn11ltz6NChtGjRIgMHDsx9992Xnj17pqqqKkmyYcOGTJky5U9jP/jggxkyZMjffv7+/fszbdq0vPDCC0mSurq6DB06NAsXLszBgwfz2WefpU2bNlm0aFHGjBmTDh06ZO7cuTn99NOPev6LFy/OBRdc0BhZw4cPT/fu3VNfX5/p06endevWRx2DE8+qVasyduzY3H777amvr88111yTp59+Ov369cttt9121N9ftmxZTj311GzdujUTJ07M+++/nyRp06ZNli9fnoaGhvTu3Ttr165Nhw4dcu+992bFihW56qqrjvepAQDNmIBJ0qdPn6xZs6bx5yPdA3PyySdn7NixmTx5cnbu3HnE1ysrK7Ns2bJceumlOfPMM7Nr166sW7cuNTU1SZJ+/fo1frt9LA4dOpTrrrsuU6ZMycUXX5wkeeCBB/LII49k9OjRWbp0aaZOnZpnnnkmF110Uc4777zs2bMnl19++VHHXrNmTV566aW8/fbbSZLvvvsub7zxRrZv3576+voMGjQoo0aNytlnn33M86ZpjR07No8++mhuuOGG9OrVK99//31jTPft2/eI9039sYNy4MCB3H333fn2229TUVGRn376qfE9f6yrX375JTt27MjVV1+dJNm3b1+6d+9+vE+LZmL+/Pl5/fXX061btzz//PNNPR2aIWuQE4F1eGQC5h/auXNnFi5cmOnTp2fq1KlHvLm9pqYmTzzxRB577LEkSdeuXfPaa6817pr8f3Zg6uvrc+ONN2bUqFEZNWpU4/GGhoZ06tQpSVJZWZk9e/YkSVavXp1Dhw6lU6dOWb58eUaOHPmX57Rx48bMmDEj7733Xtq2bds4bvv27dOmTZskv3/bvm/fvqNeH048bdq0yZNPPpnk9x27s846K1988UX69u2bzz//PF26dEmSdOjQITt37kxlZWW++uqr3HTTTVm5cmUqKiry8ccfZ8uWLYeto4qKiiRJp06dcv755+edd95Ju3btkvwe2/CfMGnSpEyaNKmpp0EzZg1yIrAOj0zA/AP19fUZO3Zs5s6dm+rq6lx//fVZsWJFRowYcdj7BgwYkNmzZ6e6ujpJ0r9//7z55pvp2bNnkqPvwMyfPz+vvPJKtm7dmrq6uixYsCCbNm3KihUrsnv37ixZsiSXXHJJ5s2bl+nTp+eOO+5Iq1atcujQoSxYsCA///xzpk2bllWrVqVVq1apq6tL7969c9ppp2X06NHZsmVLNm/enOHDh+fhhx/OuHHjkqQxjJ566qn06dMnVVVVqa6uTkNDQwYPHuxb9UItXbo0L774Ylq0aJHOnTtn+vTpGT9+fM4444zG+E1+f0relVdemR49eqSysjLJ72t11qxZqaurS//+/Y84fosWLTJ79uyMHDkyDQ0NadmyZebMmZNevXr9V84PAGieWjS46xaanSVLluSHH35ovJEfAKAUnkIGAAAUww4MAABQDDswAABAMQQMAABQDAEDAAAU44R+jPL7//NxU0+B/6IhDwxo6in8SdtLPXu9OTmwaX5TT+GIrMPmxTrkRHAirkNrsHn5uzVoBwYAACiGgAEAAIohYAAAgGIIGAAAoBgCBgAAKIaAAQAAiiFgAACAYggYAACgGAIGAAAohoABAACKIWAAAIBiCBgAAKAYAgYAACiGgAEAAIohYAAAgGIIGAAAoBgCBgAAKIaAAQAAiiFgAACAYggYAACgGAIGAAAohoABAACKIWAAAIBiCBgAAKAYrZp6An/nlTvWNvUUjqvrF9Q29RQ4in9/Pr+pp3Bc/euySU09BQCAY2IHBgAAKIaAAQAAiiFgAACAYggYAACgGAIGAAAohoABAACKIWAAAIBiCBgAAKAYAgYAACiGgAEAAIohYAAAgGIIGAAAoBgCBgAAKIaAAQAAiiFgAACAYggYAACgGAIGAAAohoABAACKIWAAAIBiCBgAAKAYAgYAACiGgAEAAIohYAAAgGIIGAAAoBgCBgAAKIaAAQAAiiFgAACAYggYAACgGAIGAAAohoABAACKIWAAAIBiCBgAAKAYAgYAACiGgAEAAIohYAAAgGIIGAAAoBgCBgAAKIaAAQAAiiFgAACAYggYAACgGAIGAAAohoABAACKIWAAAIBiCBgAAKAYAgYAACiGgAEAAIohYAAAgGIIGAAAoBgCBgAAKIaAAQAAiiFgAACAYggYAACgGAIGAAAohoABAACKIWAAAIBiCBgAAKAYAgYAACiGgAEAAIohYAAAgGIIGAAAoBitmnoCf+f6BbVNPQWauX9dNqmppwAAwP9hBwYAACiGgAEAAIohYAAAgGIIGAAAoBgCBgAAKIaAAQAAiiFgAACAYggYAACgGAIGAAAohoABAACKIWAAAIBiCBgAAKAYAgYAACiGgAEAAIohYAAAgGIIGAAAoBgCBgAAKIaAAQAAiiFgAACAYggYAACgGAIGAAAohoABAACKIWAAAIBiCBgAAKAYAgYAACiGgAEAAIohYAAAgGIIGAAAoBgCBgAAKIaAAQAAiiFgAACAYggYAACgGAIGAAAohoABAACKIWAAAIBiCBgAAKAYAgYAAChGi4aGhoamngQAAMA/YQcGAAAohoABAACKIWAAAIBiCBgAAKAYAgYAACiGgAEAAIohYAAAgGIIGAAAoBgCBgAAKIaAAQAAiiFgAACAYggYAACgGAIGAAAohoABAACKIWAAAIBiCBgAAKAYAgYAACiGgAEAAIohYAAAgGIIGAAAoBgCBgAAKIaAAQAAiiFgAACAYggYAACgGAIGAAAoxv8C8z1tY2J+2sIAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 1008x360 with 5 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1008x360 with 5 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1008x360 with 5 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load and display random samples\n",
    "image_ids = np.random.choice(dataset_train.image_ids, 4)\n",
    "for image_id in image_ids:\n",
    "    image = dataset_train.load_image(image_id)\n",
    "    mask, class_ids = dataset_train.load_mask(image_id)\n",
    "    NN.RCNN.mask.visualize.display_top_masks(image, mask, class_ids, dataset_train.class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_DIR = os.path.abspath('../../../outputs/example-RCNN-mask-train-on-shapes-dataset')\n",
    "if not os.path.isdir(MODEL_DIR):\n",
    "    os.makedirs(MODEL_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "importlib.reload(NN.RCNN.mask.model)\n",
    "importlib.reload(NN.RCNN.mask.utils)\n",
    "model = NN.RCNN.mask.model.MaskRCNN(mode=\"training\", \n",
    "                                    config=config,\n",
    "                                    model_dir=MODEL_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Which weights to start with?\n",
    "init_with = \"coco\"  # imagenet, coco, or last\n",
    "\n",
    "if init_with == \"imagenet\":\n",
    "    model.load_weights(model.get_imagenet_weights(), by_name=True)\n",
    "elif init_with == \"coco\":\n",
    "    # Load weights trained on MS COCO, but skip layers that\n",
    "    # are different due to the different number of classes\n",
    "    # See README for instructions to download the COCO weights\n",
    "    \n",
    "    # Local path to trained weights file\n",
    "    COCO_MODEL_PATH = os.path.join(MODEL_DIR, \"mask_rcnn_coco.h5\")\n",
    "    # Download COCO trained weights from Releases if needed\n",
    "    if not os.path.exists(COCO_MODEL_PATH):\n",
    "        NN.RCNN.mask.utils.download_trained_weights(COCO_MODEL_PATH)\n",
    "    \n",
    "    model.load_weights(COCO_MODEL_PATH, by_name=True,\n",
    "                       exclude=[\"mrcnn_class_logits\", \"mrcnn_bbox_fc\", \n",
    "                                \"mrcnn_bbox\", \"mrcnn_mask\"])\n",
    "elif init_with == \"last\":\n",
    "    # Load the last model you trained and continue training\n",
    "    model.load_weights(model.find_last(), by_name=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "Train in two stages:\n",
    "1. Only the heads. Here we're freezing all the backbone layers and training only the randomly initialized layers (i.e. the ones that we didn't use pre-trained weights from MS COCO). To train only the head layers, pass `layers='heads'` to the `train()` function.\n",
    "\n",
    "2. Fine-tune all layers. For this simple example it's not necessary, but we're including it to show the process. Simply pass `layers=\"all` to train all layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rpn_class_loss': 1.0,\n",
       " 'rpn_bbox_loss': 1.0,\n",
       " 'mrcnn_class_loss': 1.0,\n",
       " 'mrcnn_bbox_loss': 1.0,\n",
       " 'mrcnn_mask_loss': 1.0}"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config.LOSS_WEIGHTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting at epoch 0. LR=0.001\n",
      "\n",
      "Checkpoint Path: /mr3_boltprod_john_t_leonard/Data_Science_Projects./ObjDetection/ouputs/example-RCNN-mask-train-on-shapes-dataset/shapes20191202T1335/mask_rcnn_shapes_{epoch:04d}.h5\n",
      "Selecting layers to train\n",
      "fpn_c5p5               (Conv2D)\n",
      "fpn_c4p4               (Conv2D)\n",
      "fpn_c3p3               (Conv2D)\n",
      "fpn_c2p2               (Conv2D)\n",
      "fpn_p5                 (Conv2D)\n",
      "fpn_p2                 (Conv2D)\n",
      "fpn_p3                 (Conv2D)\n",
      "fpn_p4                 (Conv2D)\n",
      "In model:  rpn_model\n",
      "    rpn_conv_shared        (Conv2D)\n",
      "    rpn_class_raw          (Conv2D)\n",
      "    rpn_bbox_pred          (Conv2D)\n",
      "mrcnn_mask_conv1       (TimeDistributed)\n",
      "mrcnn_mask_bn1         (TimeDistributed)\n",
      "mrcnn_mask_conv2       (TimeDistributed)\n",
      "mrcnn_mask_bn2         (TimeDistributed)\n",
      "mrcnn_class_conv1      (TimeDistributed)\n",
      "mrcnn_class_bn1        (TimeDistributed)\n",
      "mrcnn_mask_conv3       (TimeDistributed)\n",
      "mrcnn_mask_bn3         (TimeDistributed)\n",
      "mrcnn_class_conv2      (TimeDistributed)\n",
      "mrcnn_class_bn2        (TimeDistributed)\n",
      "mrcnn_mask_conv4       (TimeDistributed)\n",
      "mrcnn_mask_bn4         (TimeDistributed)\n",
      "mrcnn_bbox_fc          (TimeDistributed)\n",
      "mrcnn_mask_deconv      (TimeDistributed)\n",
      "mrcnn_class_logits     (TimeDistributed)\n",
      "mrcnn_mask             (TimeDistributed)\n",
      "Tensor(\"Mean_17:0\", shape=(), dtype=float32)\n",
      "1.0\n",
      "<class 'tensorflow.python.framework.ops.Tensor'>\n",
      "Tensor(\"mul_14:0\", shape=(), dtype=float32)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'ListWrapper'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-83-83fb3ae74319>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m             \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLEARNING_RATE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m             \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m             layers='heads')\n\u001b[0m",
      "\u001b[0;32m/mr3_boltprod_john_t_leonard/Data_Science_Projects./ObjDetection/JLpyUtils/JLpyUtils/ML/NeuralNet/RCNN/mask/model.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, train_dataset, val_dataset, learning_rate, epochs, layers, augmentation, custom_callbacks, no_augmentation_sources)\u001b[0m\n\u001b[1;32m   2361\u001b[0m         \u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Checkpoint Path: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheckpoint_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2362\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trainable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2363\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLEARNING_MOMENTUM\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2365\u001b[0m         \u001b[0;31m# Work-around for Windows: Keras fails on Windows when using\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mr3_boltprod_john_t_leonard/Data_Science_Projects./ObjDetection/JLpyUtils/JLpyUtils/ML/NeuralNet/RCNN/mask/model.py\u001b[0m in \u001b[0;36mcompile\u001b[0;34m(self, learning_rate, momentum)\u001b[0m\n\u001b[1;32m   2182\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2183\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2184\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2185\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2186\u001b[0m         \u001b[0;31m# Add L2 Regularization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36madd_loss\u001b[0;34m(self, losses, inputs)\u001b[0m\n\u001b[1;32m   1130\u001b[0m       \u001b[0;32mfor\u001b[0m \u001b[0msymbolic_loss\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msymbolic_losses\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1131\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'_is_graph_network'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1132\u001b[0;31m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph_network_add_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msymbolic_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1133\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m           \u001b[0;31m# Possible a loss was added in a Layer's `build`.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/network.py\u001b[0m in \u001b[0;36m_graph_network_add_loss\u001b[0;34m(self, symbolic_loss)\u001b[0m\n\u001b[1;32m   1431\u001b[0m     \u001b[0mnew_nodes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madd_loss_layer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minbound_nodes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1432\u001b[0m     \u001b[0mnew_layers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madd_loss_layer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1433\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_insert_layers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_layers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_nodes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1434\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1435\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_graph_network_add_metric\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maggregation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/network.py\u001b[0m in \u001b[0;36m_insert_layers\u001b[0;34m(self, layers, relevant_nodes)\u001b[0m\n\u001b[1;32m   1390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1391\u001b[0m     \u001b[0;31m# Insert layers and update other layer attrs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1392\u001b[0;31m     \u001b[0mlayer_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_layers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1393\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1394\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlayer_set\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/training/tracking/data_structures.py\u001b[0m in \u001b[0;36m__hash__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    545\u001b[0m     \u001b[0;31m# List wrappers need to compare like regular lists, and so like regular\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m     \u001b[0;31m# lists they don't belong in hash tables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 547\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"unhashable type: 'ListWrapper'\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    548\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0minsert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: unhashable type: 'ListWrapper'"
     ]
    }
   ],
   "source": [
    "# Train the head branches\n",
    "# Passing layers=\"heads\" freezes all layers except the head\n",
    "# layers. You can also pass a regular expression to select\n",
    "# which layers to train by name pattern.\n",
    "model.train(dataset_train, dataset_val, \n",
    "            learning_rate=config.LEARNING_RATE, \n",
    "            epochs=1, \n",
    "            layers='heads')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
