{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is based off of [matterport's train_shapes.ipynb example](https://github.com/matterport/Mask_RCNN/blob/master/samples/shapes/train_shapes.ipynb)\n",
    "\n",
    "The notebook shows how to train Mask R-CNN on your own dataset. To keep things simple we use a synthetic dataset of shapes (squares, triangles, and circles) which enables fast training. You'd still need a GPU, though, because the network backbone is a Resnet101, which would be too slow to train on a CPU. On a GPU, you can start to get okay-ish results in a few minutes, and good results in less than an hour.\n",
    "\n",
    "The code of the *Shapes* dataset is included below. It generates images on the fly, so it doesn't require downloading any data. And it can generate images of any size, so we pick a small image size to train faster. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install Libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade JLpyUtils\n",
    "import IPython.display\n",
    "IPython.display.clear_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "import skimage.io\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import importlib\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.insert(0, '/mr3_boltprod_john_t_leonard/Data_Science_Projects./ObjDetection/pyDSlib/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyDSlib as DS\n",
    "import pyDSlib.ML.NeuralNet as NN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Configurations:\n",
      "BACKBONE                       resnet101\n",
      "BACKBONE_STRIDES               [4, 8, 16, 32, 64]\n",
      "BATCH_SIZE                     8\n",
      "BBOX_STD_DEV                   [0.1 0.1 0.2 0.2]\n",
      "COMPUTE_BACKBONE_SHAPE         None\n",
      "DETECTION_MAX_INSTANCES        100\n",
      "DETECTION_MIN_CONFIDENCE       0.7\n",
      "DETECTION_NMS_THRESHOLD        0.3\n",
      "FPN_CLASSIF_FC_LAYERS_SIZE     1024\n",
      "GPU_COUNT                      1\n",
      "GRADIENT_CLIP_NORM             5.0\n",
      "IMAGES_PER_GPU                 8\n",
      "IMAGE_CHANNEL_COUNT            3\n",
      "IMAGE_MAX_DIM                  128\n",
      "IMAGE_META_SIZE                16\n",
      "IMAGE_MIN_DIM                  128\n",
      "IMAGE_MIN_SCALE                0\n",
      "IMAGE_RESIZE_MODE              square\n",
      "IMAGE_SHAPE                    [128 128   3]\n",
      "LEARNING_MOMENTUM              0.9\n",
      "LEARNING_RATE                  0.001\n",
      "LOSS_WEIGHTS                   {'rpn_class_loss': 1.0, 'rpn_bbox_loss': 1.0, 'mrcnn_class_loss': 1.0, 'mrcnn_bbox_loss': 1.0, 'mrcnn_mask_loss': 1.0}\n",
      "MASK_POOL_SIZE                 14\n",
      "MASK_SHAPE                     [28, 28]\n",
      "MAX_GT_INSTANCES               100\n",
      "MEAN_PIXEL                     [123.7 116.8 103.9]\n",
      "MINI_MASK_SHAPE                (56, 56)\n",
      "NAME                           shapes\n",
      "NUM_CLASSES                    4\n",
      "POOL_SIZE                      7\n",
      "POST_NMS_ROIS_INFERENCE        1000\n",
      "POST_NMS_ROIS_TRAINING         2000\n",
      "PRE_NMS_LIMIT                  6000\n",
      "ROI_POSITIVE_RATIO             0.33\n",
      "RPN_ANCHOR_RATIOS              [0.5, 1, 2]\n",
      "RPN_ANCHOR_SCALES              (8, 16, 32, 64, 128)\n",
      "RPN_ANCHOR_STRIDE              1\n",
      "RPN_BBOX_STD_DEV               [0.1 0.1 0.2 0.2]\n",
      "RPN_NMS_THRESHOLD              0.7\n",
      "RPN_TRAIN_ANCHORS_PER_IMAGE    256\n",
      "STEPS_PER_EPOCH                100\n",
      "TOP_DOWN_PYRAMID_SIZE          256\n",
      "TRAIN_BN                       False\n",
      "TRAIN_ROIS_PER_IMAGE           32\n",
      "USE_MINI_MASK                  True\n",
      "USE_RPN_ROIS                   True\n",
      "VALIDATION_STEPS               5\n",
      "WEIGHT_DECAY                   0.0001\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class ShapesConfig(NN.RCNN.mask.Config):\n",
    "    \"\"\"\n",
    "    Configuration for training on the toy shapes dataset.\n",
    "    Derives from the base Config class and overrides values specific\n",
    "    to the toy shapes dataset.\n",
    "    \"\"\"\n",
    "    # Give the configuration a recognizable name\n",
    "    NAME = \"shapes\"\n",
    "\n",
    "    # Train on 1 GPU and 8 images per GPU. We can put multiple images on each\n",
    "    # GPU because the images are small. Batch size is 8 (GPUs * images/GPU).\n",
    "    GPU_COUNT = 1\n",
    "    IMAGES_PER_GPU = 8\n",
    "\n",
    "    # Number of classes (including background)\n",
    "    NUM_CLASSES = 1 + 3  # background + 3 shapes\n",
    "\n",
    "    # Use small images for faster training. Set the limits of the small side\n",
    "    # the large side, and that determines the image shape.\n",
    "    IMAGE_MIN_DIM = 128\n",
    "    IMAGE_MAX_DIM = 128\n",
    "\n",
    "    # Use smaller anchors because our image and objects are small\n",
    "    RPN_ANCHOR_SCALES = (8, 16, 32, 64, 128)  # anchor side in pixels\n",
    "\n",
    "    # Reduce training ROIs per image because the images are small and have\n",
    "    # few objects. Aim to allow ROI sampling to pick 33% positive ROIs.\n",
    "    TRAIN_ROIS_PER_IMAGE = 32\n",
    "\n",
    "    # Use a small epoch since the data is simple\n",
    "    STEPS_PER_EPOCH = 100\n",
    "\n",
    "    # use small validation steps since the epoch is small\n",
    "    VALIDATION_STEPS = 5\n",
    "    \n",
    "config = ShapesConfig()\n",
    "config.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook Preferences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ax(rows=1, cols=1, size=8):\n",
    "    \"\"\"Return a Matplotlib Axes array to be used in\n",
    "    all visualizations in the notebook. Provide a\n",
    "    central point to control graph sizes.\n",
    "    \n",
    "    Change the default size attribute to control the size\n",
    "    of rendered images\n",
    "    \"\"\"\n",
    "    _, ax = plt.subplots(rows, cols, figsize=(size*cols, size*rows))\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "Create a synthetic dataset\n",
    "\n",
    "Extend the Dataset class and add a method to load the shapes dataset, `load_shapes()`, and override the following methods:\n",
    "\n",
    "* load_image()\n",
    "* load_mask()\n",
    "* image_reference()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShapesDataset(NN.RCNN.mask.utils.Dataset):\n",
    "    \"\"\"Generates the shapes synthetic dataset. The dataset consists of simple\n",
    "    shapes (triangles, squares, circles) placed randomly on a blank surface.\n",
    "    The images are generated on the fly. No file access required.\n",
    "    \"\"\"\n",
    "\n",
    "    def load_shapes(self, count, height, width):\n",
    "        \"\"\"Generate the requested number of synthetic images.\n",
    "        count: number of images to generate.\n",
    "        height, width: the size of the generated images.\n",
    "        \"\"\"\n",
    "        # Add classes\n",
    "        self.add_class(\"shapes\", 1, \"square\")\n",
    "        self.add_class(\"shapes\", 2, \"circle\")\n",
    "        self.add_class(\"shapes\", 3, \"triangle\")\n",
    "\n",
    "        # Add images\n",
    "        # Generate random specifications of images (i.e. color and\n",
    "        # list of shapes sizes and locations). This is more compact than\n",
    "        # actual images. Images are generated on the fly in load_image().\n",
    "        for i in range(count):\n",
    "            bg_color, shapes = self.random_image(height, width)\n",
    "            self.add_image(\"shapes\", image_id=i, path=None,\n",
    "                           width=width, height=height,\n",
    "                           bg_color=bg_color, shapes=shapes)\n",
    "\n",
    "    def load_image(self, image_id):\n",
    "        \"\"\"Generate an image from the specs of the given image ID.\n",
    "        Typically this function loads the image from a file, but\n",
    "        in this case it generates the image on the fly from the\n",
    "        specs in image_info.\n",
    "        \"\"\"\n",
    "        info = self.image_info[image_id]\n",
    "        bg_color = np.array(info['bg_color']).reshape([1, 1, 3])\n",
    "        image = np.ones([info['height'], info['width'], 3], dtype=np.uint8)\n",
    "        image = image * bg_color.astype(np.uint8)\n",
    "        for shape, color, dims in info['shapes']:\n",
    "            image = self.draw_shape(image, shape, dims, color)\n",
    "        return image\n",
    "\n",
    "    def image_reference(self, image_id):\n",
    "        \"\"\"Return the shapes data of the image.\"\"\"\n",
    "        info = self.image_info[image_id]\n",
    "        if info[\"source\"] == \"shapes\":\n",
    "            return info[\"shapes\"]\n",
    "        else:\n",
    "            super(self.__class__).image_reference(self, image_id)\n",
    "\n",
    "    def load_mask(self, image_id):\n",
    "        \"\"\"Generate instance masks for shapes of the given image ID.\n",
    "        \"\"\"\n",
    "        info = self.image_info[image_id]\n",
    "        shapes = info['shapes']\n",
    "        count = len(shapes)\n",
    "        mask = np.zeros([info['height'], info['width'], count], dtype=np.uint8)\n",
    "        for i, (shape, _, dims) in enumerate(info['shapes']):\n",
    "            mask[:, :, i:i+1] = self.draw_shape(mask[:, :, i:i+1].copy(),\n",
    "                                                shape, dims, 1)\n",
    "        # Handle occlusions\n",
    "        occlusion = np.logical_not(mask[:, :, -1]).astype(np.uint8)\n",
    "        for i in range(count-2, -1, -1):\n",
    "            mask[:, :, i] = mask[:, :, i] * occlusion\n",
    "            occlusion = np.logical_and(occlusion, np.logical_not(mask[:, :, i]))\n",
    "        # Map class names to class IDs.\n",
    "        class_ids = np.array([self.class_names.index(s[0]) for s in shapes])\n",
    "        return mask.astype(np.bool), class_ids.astype(np.int32)\n",
    "\n",
    "    def draw_shape(self, image, shape, dims, color):\n",
    "        \"\"\"Draws a shape from the given specs.\"\"\"\n",
    "        # Get the center x, y and the size s\n",
    "        x, y, s = dims\n",
    "        if shape == 'square':\n",
    "            cv2.rectangle(image, (x-s, y-s), (x+s, y+s), color, -1)\n",
    "        elif shape == \"circle\":\n",
    "            cv2.circle(image, (x, y), s, color, -1)\n",
    "        elif shape == \"triangle\":\n",
    "            points = np.array([[(x, y-s),\n",
    "                                (x-s/math.sin(math.radians(60)), y+s),\n",
    "                                (x+s/math.sin(math.radians(60)), y+s),\n",
    "                                ]], dtype=np.int32)\n",
    "            cv2.fillPoly(image, points, color)\n",
    "        return image\n",
    "\n",
    "    def random_shape(self, height, width):\n",
    "        \"\"\"Generates specifications of a random shape that lies within\n",
    "        the given height and width boundaries.\n",
    "        Returns a tuple of three valus:\n",
    "        * The shape name (square, circle, ...)\n",
    "        * Shape color: a tuple of 3 values, RGB.\n",
    "        * Shape dimensions: A tuple of values that define the shape size\n",
    "                            and location. Differs per shape type.\n",
    "        \"\"\"\n",
    "        # Shape\n",
    "        shape = random.choice([\"square\", \"circle\", \"triangle\"])\n",
    "        # Color\n",
    "        color = tuple([random.randint(0, 255) for _ in range(3)])\n",
    "        # Center x, y\n",
    "        buffer = 20\n",
    "        y = random.randint(buffer, height - buffer - 1)\n",
    "        x = random.randint(buffer, width - buffer - 1)\n",
    "        # Size\n",
    "        s = random.randint(buffer, height//4)\n",
    "        return shape, color, (x, y, s)\n",
    "\n",
    "    def random_image(self, height, width):\n",
    "        \"\"\"Creates random specifications of an image with multiple shapes.\n",
    "        Returns the background color of the image and a list of shape\n",
    "        specifications that can be used to draw the image.\n",
    "        \"\"\"\n",
    "        # Pick random background color\n",
    "        bg_color = np.array([random.randint(0, 255) for _ in range(3)])\n",
    "        # Generate a few random shapes and record their\n",
    "        # bounding boxes\n",
    "        shapes = []\n",
    "        boxes = []\n",
    "        N = random.randint(1, 4)\n",
    "        for _ in range(N):\n",
    "            shape, color, dims = self.random_shape(height, width)\n",
    "            shapes.append((shape, color, dims))\n",
    "            x, y, s = dims\n",
    "            boxes.append([y-s, x-s, y+s, x+s])\n",
    "        # Apply non-max suppression wit 0.3 threshold to avoid\n",
    "        # shapes covering each other\n",
    "        keep_ixs = NN.RCNN.mask.utils.non_max_suppression(np.array(boxes), np.arange(N), 0.3)\n",
    "        shapes = [s for i, s in enumerate(shapes) if i in keep_ixs]\n",
    "        return bg_color, shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training dataset\n",
    "dataset_train = ShapesDataset()\n",
    "dataset_train.load_shapes(500, config.IMAGE_SHAPE[0], config.IMAGE_SHAPE[1])\n",
    "dataset_train.prepare()\n",
    "\n",
    "# Validation dataset\n",
    "dataset_val = ShapesDataset()\n",
    "dataset_val.load_shapes(50, config.IMAGE_SHAPE[0], config.IMAGE_SHAPE[1])\n",
    "dataset_val.prepare()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1008x360 with 5 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1008x360 with 5 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1008x360 with 5 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1008x360 with 5 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load and display random samples\n",
    "image_ids = np.random.choice(dataset_train.image_ids, 4)\n",
    "for image_id in image_ids:\n",
    "    image = dataset_train.load_image(image_id)\n",
    "    mask, class_ids = dataset_train.load_mask(image_id)\n",
    "    NN.RCNN.mask.visualize.display_top_masks(image, mask, class_ids, dataset_train.class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_DIR = os.path.abspath('../../../outputs/example-RCNN-mask-train-on-shapes-dataset')\n",
    "if not os.path.isdir(MODEL_DIR):\n",
    "    os.makedirs(MODEL_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model = NN.RCNN.mask.model.MaskRCNN(mode=\"training\", \n",
    "                                    config=config,\n",
    "                                    model_dir=MODEL_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading pretrained model to /mr3_boltprod_john_t_leonard/Data_Science_Projects./ObjDetection/outputs/example-RCNN-mask-train-on-shapes-dataset/mask_rcnn_coco.h5 ...\n",
      "... done downloading pretrained model!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Which weights to start with?\n",
    "init_with = \"coco\"  # imagenet, coco, or last\n",
    "\n",
    "if init_with == \"imagenet\":\n",
    "    model.load_weights(model.get_imagenet_weights(), by_name=True)\n",
    "elif init_with == \"coco\":\n",
    "    # Load weights trained on MS COCO, but skip layers that\n",
    "    # are different due to the different number of classes\n",
    "    # See README for instructions to download the COCO weights\n",
    "    \n",
    "    # Local path to trained weights file\n",
    "    COCO_MODEL_PATH = os.path.join(MODEL_DIR, \"mask_rcnn_coco.h5\")\n",
    "    # Download COCO trained weights from Releases if needed\n",
    "    if not os.path.exists(COCO_MODEL_PATH):\n",
    "        NN.RCNN.mask.utils.download_trained_weights(COCO_MODEL_PATH)\n",
    "    \n",
    "    model.load_weights(COCO_MODEL_PATH, by_name=True,\n",
    "                       exclude=[\"mrcnn_class_logits\", \"mrcnn_bbox_fc\", \n",
    "                                \"mrcnn_bbox\", \"mrcnn_mask\"])\n",
    "elif init_with == \"last\":\n",
    "    # Load the last model you trained and continue training\n",
    "    model.load_weights(model.find_last(), by_name=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "Train in two stages:\n",
    "1. Only the heads. Here we're freezing all the backbone layers and training only the randomly initialized layers (i.e. the ones that we didn't use pre-trained weights from MS COCO). To train only the head layers, pass `layers='heads'` to the `train()` function.\n",
    "\n",
    "2. Fine-tune all layers. For this simple example it's not necessary, but we're including it to show the process. Simply pass `layers=\"all` to train all layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rpn_class_loss': 1.0,\n",
       " 'rpn_bbox_loss': 1.0,\n",
       " 'mrcnn_class_loss': 1.0,\n",
       " 'mrcnn_bbox_loss': 1.0,\n",
       " 'mrcnn_mask_loss': 1.0}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config.LOSS_WEIGHTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting at epoch 0. LR=0.001\n",
      "\n",
      "Checkpoint Path: /mr3_boltprod_john_t_leonard/Data_Science_Projects./ObjDetection/outputs/example-RCNN-mask-train-on-shapes-dataset/shapes20191210T0221/mask_rcnn_shapes_{epoch:04d}.h5\n",
      "Selecting layers to train\n",
      "fpn_c5p5               (Conv2D)\n",
      "fpn_c4p4               (Conv2D)\n",
      "fpn_c3p3               (Conv2D)\n",
      "fpn_c2p2               (Conv2D)\n",
      "fpn_p5                 (Conv2D)\n",
      "fpn_p2                 (Conv2D)\n",
      "fpn_p3                 (Conv2D)\n",
      "fpn_p4                 (Conv2D)\n",
      "In model:  rpn_model\n",
      "    rpn_conv_shared        (Conv2D)\n",
      "    rpn_class_raw          (Conv2D)\n",
      "    rpn_bbox_pred          (Conv2D)\n",
      "mrcnn_mask_conv1       (TimeDistributed)\n",
      "mrcnn_mask_bn1         (TimeDistributed)\n",
      "mrcnn_mask_conv2       (TimeDistributed)\n",
      "mrcnn_mask_bn2         (TimeDistributed)\n",
      "mrcnn_class_conv1      (TimeDistributed)\n",
      "mrcnn_class_bn1        (TimeDistributed)\n",
      "mrcnn_mask_conv3       (TimeDistributed)\n",
      "mrcnn_mask_bn3         (TimeDistributed)\n",
      "mrcnn_class_conv2      (TimeDistributed)\n",
      "mrcnn_class_bn2        (TimeDistributed)\n",
      "mrcnn_mask_conv4       (TimeDistributed)\n",
      "mrcnn_mask_bn4         (TimeDistributed)\n",
      "mrcnn_bbox_fc          (TimeDistributed)\n",
      "mrcnn_mask_deconv      (TimeDistributed)\n",
      "mrcnn_class_logits     (TimeDistributed)\n",
      "mrcnn_mask             (TimeDistributed)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'ListWrapper'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-83fb3ae74319>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m             \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLEARNING_RATE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m             \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m             layers='heads')\n\u001b[0m",
      "\u001b[0;32m/mr3_boltprod_john_t_leonard/Data_Science_Projects./ObjDetection/pyDSlib/pyDSlib/ML/NeuralNet/RCNN/mask/model.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, train_dataset, val_dataset, learning_rate, epochs, layers, augmentation, custom_callbacks, no_augmentation_sources)\u001b[0m\n\u001b[1;32m   2355\u001b[0m         \u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Checkpoint Path: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheckpoint_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2356\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trainable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2357\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLEARNING_MOMENTUM\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2358\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2359\u001b[0m         \u001b[0;31m# Work-around for Windows: Keras fails on Windows when using\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mr3_boltprod_john_t_leonard/Data_Science_Projects./ObjDetection/pyDSlib/pyDSlib/ML/NeuralNet/RCNN/mask/model.py\u001b[0m in \u001b[0;36mcompile\u001b[0;34m(self, learning_rate, momentum)\u001b[0m\n\u001b[1;32m   2176\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce_mean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLOSS_WEIGHTS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2178\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2180\u001b[0m         \u001b[0;31m# Add L2 Regularization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36madd_loss\u001b[0;34m(self, losses, inputs)\u001b[0m\n\u001b[1;32m   1130\u001b[0m       \u001b[0;32mfor\u001b[0m \u001b[0msymbolic_loss\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msymbolic_losses\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1131\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'_is_graph_network'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1132\u001b[0;31m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph_network_add_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msymbolic_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1133\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m           \u001b[0;31m# Possible a loss was added in a Layer's `build`.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/network.py\u001b[0m in \u001b[0;36m_graph_network_add_loss\u001b[0;34m(self, symbolic_loss)\u001b[0m\n\u001b[1;32m   1431\u001b[0m     \u001b[0mnew_nodes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madd_loss_layer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minbound_nodes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1432\u001b[0m     \u001b[0mnew_layers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madd_loss_layer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1433\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_insert_layers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_layers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_nodes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1434\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1435\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_graph_network_add_metric\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maggregation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/network.py\u001b[0m in \u001b[0;36m_insert_layers\u001b[0;34m(self, layers, relevant_nodes)\u001b[0m\n\u001b[1;32m   1390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1391\u001b[0m     \u001b[0;31m# Insert layers and update other layer attrs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1392\u001b[0;31m     \u001b[0mlayer_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_layers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1393\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1394\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlayer_set\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/training/tracking/data_structures.py\u001b[0m in \u001b[0;36m__hash__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    545\u001b[0m     \u001b[0;31m# List wrappers need to compare like regular lists, and so like regular\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m     \u001b[0;31m# lists they don't belong in hash tables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 547\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"unhashable type: 'ListWrapper'\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    548\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0minsert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: unhashable type: 'ListWrapper'"
     ]
    }
   ],
   "source": [
    "# Train the head branches\n",
    "# Passing layers=\"heads\" freezes all layers except the head\n",
    "# layers. You can also pass a regular expression to select\n",
    "# which layers to train by name pattern.\n",
    "model.train(dataset_train, dataset_val, \n",
    "            learning_rate=config.LEARNING_RATE, \n",
    "            epochs=1, \n",
    "            layers='heads')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
