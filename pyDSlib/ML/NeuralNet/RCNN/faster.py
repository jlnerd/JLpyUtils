import numpy as _np

class Config:
    """
    Configuration class for RCNN.faster
    
    Arguments:
    ----------
        verbose: int. print-out verbosity
        backbone: Conv2D backbone model to be used
        use_horizontal_flips: horizontal flip data augmentation boolean
        use_vertical_flips: vertical flip data augmentation boolean
        rot_90: 90 degree rotation data augmentation boolean
        img_size: Size to resize the smallest side of the image
            - Original setting in paper: 600
            - Reducing to 300 will save training time.
        anchor_box_scales: Anchor box scales
            - Note that if img_size is smaller, anchor_box_scales should be scaled
                - i.e.: img_size = 300 -> anchor_box_scales = [64, 128, 256] 
            - Original anchor_box_scales in faster RCNN paper: [128, 256, 512]
        anchor_box_ratios: ...
        img_channel_mean: image channel-wise mean to subtract
        img_scaling_factor: ...
        n_rois: Number of ROIS at once
        RPN_stride: Region proposal network strides (depends on backbone)
        RPN_ROI_min_overlap: ...
        RPN_ROI_max_overlap: ...
        balanced_classes: boolean of wheter classes are balanced
        std_scaling: scaling the stdev
        classifier_regr_std: ...
        classifier_ROI_min_overlap: ...
        classifier_ROI_max_overlap: ...
        class_mapping: placeholder for the class mapping, automatically generated by the parser
        model_path: directory where model will be saved):
    """
    def __init__(self,
               verbose = 1,
               backbone = 'VGG16',
               use_horizontal_flips=False,
               use_vertical_flips=False,
               rot_90=False,
               img_size = 600,
               anchor_box_scales = [128, 256, 512],
               anchor_box_ratios = [[1,1],
                                    [1./_np.sqrt(2), 2./_np.sqrt(2)],
                                    [2./_np.sqrt(2), 1./_np.sqrt(2)]],
               img_channel_mean = [103.939, 116.779, 123.68],
               img_scaling_factor = 1.0,
               n_rois = 4,
               RPN_stride=16,
               RPN_ROI_min_overlap = 0.3,
               RPN_ROI_max_overlap = 0.7,
               balanced_classes=False,
               std_scaling=4.0,
               classifier_regr_std = [8.0, 8.0, 4.0, 4.0],
               classifier_ROI_min_overlap = 0.1,
               classifier_ROI_max_overlap = 0.5,
               class_mapping = None,
               model_path = None):
        
        self.verbose = verbose
        self.backbone = backbone
        self.use_horizontal_flips=use_horizontal_flips
        self.use_vertical_flips=use_vertical_flips
        self.rot_90=rot_90
        self.img_size = img_size
        self.anchor_box_scales = anchor_box_scales
        self.anchor_box_ratios = anchor_box_ratios
        self.img_channel_mean = img_channel_mean
        self.img_scaling_factor = img_scaling_factor
        self.n_rois = n_rois
        self.RPN_stride = RPN_stride 
        self.RPN_ROI_min_overlap = RPN_ROI_min_overlap 
        self.RPN_ROI_max_overlap = RPN_ROI_max_overlap 
        self.balanced_classes = balanced_classes
        self.std_scaling = std_scaling
        self.classifier_regr_std = classifier_regr_std 
        self.classifier_ROI_min_overlap = classifier_ROI_min_overlap
        self.classifier_ROI_max_overlap = classifier_ROI_max_overlap 
        self.class_mapping = class_mapping
        self.model_path = model_path
        
               
                                    